{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eea3359",
   "metadata": {},
   "source": [
    "# categorize_transactions.py (notebook)\n",
    "\n",
    "This notebook is a cell-by-cell conversion of the script `categorize_transactions.py`.\n",
    "Run the code cells in order. Cells contain the same functions and logic as the original script, plus a small example showing how to run the `main` workflow from inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a809833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Try rapidfuzz first; if unavailable, fall back to difflib\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz  # type: ignore\n",
    "    HAVE_RAPIDFUZZ = True\n",
    "except Exception:\n",
    "    import difflib\n",
    "    HAVE_RAPIDFUZZ = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_RE = re.compile(r\"[^\\w\\s]\")\n",
    "SPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if pd.isna(text) else str(text)\n",
    "    t = text.strip().lower()\n",
    "    t = PUNCT_RE.sub(\" \", t)         # remove punctuation\n",
    "    t = SPACE_RE.sub(\" \", t)         # collapse spaces\n",
    "    return t.strip()\n",
    "\n",
    "def most_common_pair(pairs: List[Tuple[str, str]]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    if not pairs:\n",
    "        return None, None\n",
    "    counter = Counter(pairs)\n",
    "    (cat, subcat), _ = counter.most_common(1)[0]\n",
    "    return cat, subcat\n",
    "\n",
    "def build_description_lookup(dest_df: pd.DataFrame) -> Dict[str, Tuple[Optional[str], Optional[str]]]:\n",
    "    # Group by normalized description and take the most common (category, sub-category) pair\n",
    "    pairs_by_desc: Dict[str, List[Tuple[str, str]]] = defaultdict(list)\n",
    "    for _, row in dest_df.iterrows():\n",
    "        nd = normalize(row.get(\"description\", \"\"))\n",
    "        cat = row.get(\"category\")\n",
    "        sub = row.get(\"sub-category\") if \"sub-category\" in row else row.get(\"sub_category\")\n",
    "        if pd.notna(cat) and pd.notna(sub):\n",
    "            pairs_by_desc[nd].append((str(cat), str(sub)))\n",
    "    lookup: Dict[str, Tuple[Optional[str], Optional[str]]] = {}\n",
    "    for nd, pairs in pairs_by_desc.items():\n",
    "        lookup[nd] = most_common_pair(pairs)\n",
    "    return lookup\n",
    "\n",
    "def fuzzy_best_match(query: str, choices: List[str], threshold: int = 90) -> Tuple[Optional[str], float]:\n",
    "    if not choices:\n",
    "        return None, 0.0\n",
    "    if HAVE_RAPIDFUZZ:\n",
    "        # Using token_set_ratio helps with word order/duplication\n",
    "        match = process.extractOne(query, choices, scorer=fuzz.token_set_ratio)\n",
    "        if match is None:\n",
    "            return None, 0.0\n",
    "        choice, score, _ = match\n",
    "        return choice, float(score)\n",
    "    else:\n",
    "        # difflib returns [best, ...]; similarity 0..1\n",
    "        best = None\n",
    "        best_score = 0.0\n",
    "        for c in choices:\n",
    "            s = difflib.SequenceMatcher(None, query, c).ratio()\n",
    "            if s > best_score:\n",
    "                best_score = s\n",
    "                best = c\n",
    "        # Map 0..1 to 0..100 to align with threshold\n",
    "        return best, best_score * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Categorize transactions by matching descriptions to a destination file (exact, then fuzzy).\")\n",
    "    parser.add_argument(\"--source\", required=True, help=\"Path to source CSV (Date, description, amount, card)\")\n",
    "    parser.add_argument(\"--destination\", required=True, help=\"Path to destination CSV (Date, description, amount, card, category, sub-category)\")\n",
    "    parser.add_argument(\"--threshold\", type=int, default=90, help=\"Fuzzy match threshold (0-100). Default: 90\")\n",
    "    parser.add_argument(\"--reviewed\", default=None, help=\"Optional path to a reviewed CSV (from to_review.csv) to apply final categories before writing updated_destination.csv\")\n",
    "    parser.add_argument(\"--output_dir\", default=\".\", help=\"Directory to write outputs\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Read CSVs (case-insensitive column handling)\n",
    "    def standardize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        lower_map = {c: c.strip().lower() for c in df.columns}\n",
    "        df = df.rename(columns=lower_map)\n",
    "        # normalize common variants\n",
    "        if \"sub_category\" in df.columns and \"sub-category\" not in df.columns:\n",
    "            df = df.rename(columns={\"sub_category\": \"sub-category\"})\n",
    "        return df\n",
    "\n",
    "    src = standardize_cols(pd.read_csv(args.source))\n",
    "    dst = standardize_cols(pd.read_csv(args.destination))\n",
    "\n",
    "    required_src = {\"date\", \"description\", \"amount\", \"card\"}\n",
    "    if not required_src.issubset(set(src.columns)):\n",
    "        raise ValueError(f\"Source file must contain columns {required_src}. Found: {list(src.columns)}\")\n",
    "\n",
    "    required_dst = {\"date\", \"description\", \"amount\", \"card\", \"category\", \"sub-category\"}\n",
    "    if not required_dst.issubset(set(dst.columns)):\n",
    "        missing = required_dst - set(dst.columns)\n",
    "        raise ValueError(f\"Destination file missing required columns: {missing}\")\n",
    "\n",
    "    # Build lookup for exact description -> (category, subcat)\n",
    "    desc_to_pair = build_description_lookup(dst)\n",
    "    all_choices = list(desc_to_pair.keys())\n",
    "\n",
    "    # Prepare output rows\n",
    "    suggestions = []\n",
    "    for _, row in src.iterrows():\n",
    "        nd = normalize(row[\"description\"])\n",
    "        cat = None\n",
    "        sub = None\n",
    "        match_type = \"none\"\n",
    "        match_desc = None\n",
    "        match_score = 0.0\n",
    "\n",
    "        # Exact\n",
    "        if nd in desc_to_pair and all(v is not None for v in desc_to_pair[nd]):\n",
    "            cat, sub = desc_to_pair[nd]\n",
    "            match_type = \"exact\"\n",
    "            match_desc = nd\n",
    "            match_score = 100.0\n",
    "        else:\n",
    "            # Fuzzy\n",
    "            best, score = fuzzy_best_match(nd, all_choices, threshold=args.threshold)\n",
    "            if best is not None and score >= args.threshold and all(v is not None for v in desc_to_pair.get(best, (None, None))):\n",
    "                cat, sub = desc_to_pair[best]\n",
    "                match_type = \"fuzzy\"\n",
    "                match_desc = best\n",
    "                match_score = score\n",
    "\n",
    "        suggestion = {\n",
    "            \"date\": row.get(\"date\"),\n",
    "            \"description\": row.get(\"description\"),\n",
    "            \"amount\": row.get(\"amount\"),\n",
    "            \"card\": row.get(\"card\"),\n",
    "            \"category\": cat,\n",
    "            \"sub-category\": sub,\n",
    "            \"match_type\": match_type,\n",
    "            \"matched_description_norm\": match_desc,\n",
    "            \"match_score\": round(match_score, 1),\n",
    "            \"needs_review\": match_type != \"exact\",  # review fuzzy and none\n",
    "            \"reason\": \"exact\" if match_type == \"exact\" else (\"fuzzy_suggest\" if match_type == \"fuzzy\" else \"no_match\"),\n",
    "        }\n",
    "        suggestions.append(suggestion)\n",
    "\n",
    "    suggest_df = pd.DataFrame(suggestions)\n",
    "\n",
    "    # Save all rows with suggestions\n",
    "    merged_path = os.path.join(args.output_dir, \"merged_with_suggestions.csv\")\n",
    "    suggest_df.to_csv(merged_path, index=False)\n",
    "\n",
    "    # Save rows needing review\n",
    "    to_review = suggest_df[suggest_df[\"needs_review\"]].copy()\n",
    "    # Provide empty columns for user to fill/override\n",
    "    to_review[\"category_final\"] = to_review[\"category\"]\n",
    "    to_review[\"sub-category_final\"] = to_review[\"sub-category\"]\n",
    "    review_path = os.path.join(args.output_dir, \"to_review.csv\")\n",
    "    to_review.to_csv(review_path, index=False)\n",
    "\n",
    "    print(f\"Wrote: {merged_path}\")\n",
    "    print(f\"Wrote: {review_path}\")\n",
    "\n",
    "    # If user provided a reviewed file, apply it and write updated_destination.csv\n",
    "    if args.reviewed:\n",
    "        reviewed = standardize_cols(pd.read_csv(args.reviewed))\n",
    "        # Expect category_final and sub-category_final\n",
    "        for col in [\"category_final\", \"sub-category_final\"]:\n",
    "            if col not in reviewed.columns:\n",
    "                raise ValueError(f\"Reviewed file must include '{col}' column.\")\n",
    "\n",
    "        # Join reviewed choices back to suggestions on (date, description, amount, card)\n",
    "        join_keys = [\"date\", \"description\", \"amount\", \"card\"]\n",
    "        merged = suggest_df.merge(\n",
    "            reviewed[join_keys + [\"category_final\", \"sub-category_final\"]],\n",
    "            on=join_keys,\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_rev\"),\n",
    "        )\n",
    "        # Prefer reviewed values when provided (not NaN)\n",
    "        def coalesce(a, b):\n",
    "            return b if pd.notna(b) and str(b).strip() != \"\" else a\n",
    "\n",
    "        merged[\"category\"] = [coalesce(a, b) for a, b in zip(merged[\"category\"], merged[\"category_final\"]) ]\n",
    "        merged[\"sub-category\"] = [coalesce(a, b) for a, b in zip(merged[\"sub-category\"], merged[\"sub-category_final\"]) ]\n",
    "\n",
    "        # Build the updated destination by appending newly categorized rows from source.\n",
    "        # Avoid duplicates: define a transaction id by (date, description, amount, card).\n",
    "        dst_keys = [\"date\", \"description\", \"amount\", \"card\", \"category\", \"sub-category\"]\n",
    "        # Source rows now with filled categories\n",
    "        finalized_src = merged.copy()\n",
    "        # Keep only required columns for destination\n",
    "        finalized_src = finalized_src[dst_keys]\n",
    "\n",
    "        # Combine with existing destination and drop duplicates\n",
    "        updated_dst = pd.concat([dst[dst_keys], finalized_src], ignore_index=True)\n",
    "        updated_dst.drop_duplicates(subset=[\"date\", \"description\", \"amount\", \"card\"], keep=\"last\", inplace=True)\n",
    "\n",
    "        out_path = os.path.join(args.output_dir, \"updated_destination.csv\")\n",
    "        updated_dst.to_csv(out_path, index=False)\n",
    "        print(f\"Wrote: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff63de5",
   "metadata": {},
   "source": [
    "## How to run in this notebook\n",
    "\n",
    "You can either run the notebook cells sequentially and then run the script-like entry point, or call `main()` from a cell after setting `sys.argv`.\n",
    "Example: set the arguments and call `main()` (the project includes `source.csv` and `destination.csv` in the workspace root):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: run the main workflow from within the notebook\n",
    "import sys\n",
    "# Adjust the paths below if your files are elsewhere\n",
    "sys.argv = [\n",
    "    'notebook',\n",
    "    '--source', 'source.csv',\n",
    "    '--destination', 'destination.csv',\n",
    "    '--output_dir', '.',\n",
    "]\n",
    "# Call main() to execute the workflow (reads/writes CSVs)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a3a2d",
   "metadata": {},
   "source": [
    "---\n",
    "Created notebook `categorize_transactions.ipynb` mirroring the script.\n",
    "Notes:\n",
    "- If `rapidfuzz` is installed it will be used for fuzzy matching; otherwise the notebook falls back to `difflib`.\n",
    "- Running the example cell will execute `main()` and produce `merged_with_suggestions.csv` and `to_review.csv` in the current directory.\n",
    "- If you prefer not to run the entire script inside the notebook, you can import and call individual functions (e.g., `build_description_lookup`) for interactive exploration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
